{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for interactive 3d plot \n",
    "!pip install PrettyTable > /dev/null 2>&1\n",
    "from prettytable import PrettyTable\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn import metrics\n",
    "import enum\n",
    "\n",
    "# import ipyvolume as ipv\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Files\n",
    "trainFile = 'trainNN.txt'\n",
    "testFile = 'testNN.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For softmax loss\n",
    "$$dZ = \\frac{dL}{dZ} = A - Y$$\n",
    "\\begin{align}\n",
    "    dW = \\frac{dL}{dW^L} &= \\frac{dL}{dZ^L} \\frac{dZ^L}{dW^L} \\\\ \n",
    "    &= (A^L−Y)\\frac{d}{dW^L}(A^{L−1}W^L+b^L) \\\\\n",
    "    &=(A^L - Y) A^{L-1} \\\\\n",
    "    db = \\frac{dL}{db} &= \\frac{dL}{dZ^L} \\frac{dZ^L}{db} \\\\ \n",
    "    &= (A^L−Y)\\frac{d}{db}(A^{L−1}W^L+b^L) \\\\\n",
    "    &=(A^L - Y) \\\\\n",
    "    dA^{L-1} = \\frac{dL}{dA^{L-1}} &= \\frac{dL}{dZ^L} \\frac{dZ^L}{dA^{L-1}} \\\\ \n",
    "    &= (A^L−Y)\\frac{d}{dA^{L-1}}(A^{L−1}W^L+b^L) \\\\\n",
    "    &=(A^L - Y) W^L \\\\\n",
    "    \\text{In general }dA^{i-1} = \\frac{dL}{dA^{i-1}} &= \\frac{dL}{dZ^i} \\frac{dZ^i}{dA^{i-1}} \\\\ \n",
    "    &= dZ^i W^i \\\\\n",
    "\\end{align}\n",
    "\n",
    "For all other layers except the layer L we can define as:\n",
    "\\begin{align}\n",
    "    dW = \\frac{dL}{dW^{L-1}}  &= \\frac{dL}{dZ^L}\\frac{dZ^L}{dA^{L−1}} \\frac{dA^{L−1}}{dZ^{L−1}}\\frac{dZ^{L−1}}{dW^{L−1}} \\\\ \n",
    "    &=  (A^L−Y)\\frac{d}{dA^{L-1}}(A^{L−1}W^{L}+b^{L}) \n",
    "    \\frac{d}{dZ^{L−1}} (σ(Z^{L−1})) \\frac{d}{dW^{L−1}}(A^{L−2}W^{L−1}+b^{L−1})\\\\\n",
    "    &= (A^L - Y) W^L \\sigma'(Z^{L−1})A^{L−2}\\\\ \n",
    "    \\text{In general } dW^{i} = dZ^{i} A^{i-1} \\\\\n",
    "     dZ^{i} = dA^{i}  \\sigma'(Z^{i})\\\\\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "def getBinarization(Y, num_class):\n",
    "    \"\"\"\n",
    "        Return num_class X sample matrix of 0,1s \n",
    "    \"\"\"\n",
    "    from sklearn.preprocessing import LabelEncoder \n",
    "    from sklearn.preprocessing import OneHotEncoder \n",
    "    enc = OneHotEncoder() \n",
    "    df = np.array(Y)\n",
    "\n",
    "    enc.fit(pd.DataFrame(df.reshape(len(df),)))\n",
    "    matrix = enc.transform(pd.DataFrame(df.reshape(len(df),))).toarray().T\n",
    "    return matrix, enc.categories_[0]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def pre_process_data(train_x, train_y, test_x, test_y):\n",
    "    #normalization \n",
    "    for i in range(numFeature):\n",
    "        train_x[:, i] = (train_x[:, i] - np.mean(train_x[:,i]))/np.std(train_x[:,i])\n",
    "        test_x[:, i] = (test_x[:, i] - np.mean(test_x[:,i]))/np.std(test_x[:,i])\n",
    "    \n",
    "    print(train_x)\n",
    "    from sklearn.preprocessing import LabelEncoder \n",
    "    from sklearn.preprocessing import OneHotEncoder \n",
    "    enc = OneHotEncoder(sparse=False, categories='auto')\n",
    "    train_y = enc.fit_transform(train_y.reshape(len(train_y), -1))\n",
    " \n",
    "    test_y = enc.transform(test_y.reshape(len(test_y), -1))\n",
    " \n",
    "    return train_x, train_y, test_x, test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod \n",
    "\n",
    "class Activation(ABC): \n",
    "\n",
    "    @abstractmethod\n",
    "    def __call__(self, Z):\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def _derivative(self, Z):\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def backward(self, dA):\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def __repr__(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear:\n",
    "    def __init__(self, row, col):\n",
    "        np.random.seed(1)\n",
    "        self.W = np.random.randn(row, col)\n",
    "        self.b = np.zeros((row, 1))\n",
    "        self.A_prev = None\n",
    "        \n",
    "    def update(self,  dW, db, m, learning_rate = 0.9 ,  l2_reg_param = 0.1):\n",
    "        self.W = self.W - learning_rate * dW  - learning_rate * l2_reg_param / m * self.W\n",
    "        self.b = self.b - learning_rate * db\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return str(self.W)\n",
    "    \n",
    "    def __call__(self, A):\n",
    "        self.A_prev = A\n",
    "        Z = self.W.dot(A) + self.b\n",
    "        assert(Z.shape == (self.W.shape[0], A.shape[1]))\n",
    "\n",
    "        return Z\n",
    "\n",
    "    def backward(self, m,  dZ=None):        \n",
    "\n",
    "        dW = dZ.dot(self.A_prev.T) / m \n",
    "        db = np.sum(dZ, axis=1, keepdims=True) / m\n",
    "\n",
    "        dA_prev = (self.W.T).dot(dZ)\n",
    "        assert (dA_prev.shape == self.A_prev.shape)\n",
    "        assert (dW.shape == self.W.shape)\n",
    "\n",
    "        return dA_prev, dW, db\n",
    "\n",
    "    \n",
    "class Sigmoid(Activation):\n",
    "    def __init__(self):\n",
    "        self.A = None\n",
    "    \n",
    "    def __call__(self, Z):\n",
    "        self.Z = Z\n",
    "        return 1/(1 + np.exp(-Z)) \n",
    "    \n",
    "    def _derivative(self, Z):\n",
    "        s = 1 / (1 + np.exp(-Z))\n",
    "        return s * (1 - s)\n",
    "\n",
    "    def backward(self, dA):\n",
    "        dZ = dA * self._derivative(self.Z)\n",
    "        return dZ\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return str(self.Z)\n",
    "\n",
    "        \n",
    "class LeakyRelu(Activation):\n",
    "    SLOPE = 0.1\n",
    "    def __init__(self):\n",
    "        self.A = None\n",
    "        self.slope = 0.1\n",
    "    \n",
    "    def __call__(self, Z):\n",
    "        self.Z = Z\n",
    "        return Z*(Z>0) + LeakyRelu.SLOPE*Z*(Z<0)\n",
    "    \n",
    "    def _derivative(self, Z):\n",
    "        return (1.0)*(Z>0)+(LeakyRelu.SLOPE)*(Z<0)\n",
    "\n",
    "    def backward(self, dAPrev):\n",
    "        dZ = dAPrev * self._derivative(self.Z)\n",
    "        return dZ\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return str(self.Z)\n",
    "    \n",
    "    \n",
    "class Softmax:\n",
    "    def __init__(self):\n",
    "        self.A = None\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        expZ = np.exp(x)\n",
    "        self.A =  expZ / expZ.sum(axis=0, keepdims=True) # activation \n",
    "        return self.A\n",
    "\n",
    "    def backward(self, Y):\n",
    "        dZ = self.A - Y\n",
    "        return dZ\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return str(self.A[:,:3])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ACTIVATION_ENUM(str, enum.Enum): \n",
    "    SIGMOID = \"sigmoid\"\n",
    "    LEAKY_RELU = \"leaky_relu\"\n",
    "    \n",
    "def get_activation(activation):\n",
    "    if activation == ACTIVATION_ENUM.SIGMOID:\n",
    "        return Sigmoid()\n",
    "    elif activation == ACTIVATION_ENUM.LEAKY_RELU:\n",
    "        return LeakyRelu()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANN:\n",
    "    \n",
    "    def __init__(self, layers_size, num_features, num_class, \n",
    "                 hidden_layer_activation, l2_regularization_param=0.1):\n",
    "        self.layers_size = layers_size\n",
    "        self.layers_size.append(num_class) # output layer \n",
    "        self.layers_size.insert(0, num_features) # input layer \n",
    "        self.l2_regularization_param = l2_regularization_param \n",
    "        \n",
    "        self.n = 0\n",
    "        self.costs = []\n",
    "        \n",
    "        self.num_features = num_features\n",
    "        self.num_class = num_class\n",
    "        self.num_layers = len(self.layers_size) - 1   # excluding input layers\n",
    "        \n",
    "        self.linear_layer = []\n",
    "        self.activation_layer = []        \n",
    "        \n",
    "        # Linear-Sigmoid -> Linear-Sigmoid ( for L-1 layers )\n",
    "        \n",
    "        #L linear layers \n",
    "        for i in range(1, self.num_layers+1):  \n",
    "            self.linear_layer.append( Linear(self.layers_size[i], self.layers_size[i-1]) )  \n",
    "        \n",
    "        for i in range(self.num_layers-1):\n",
    "            self.activation_layer.append( get_activation(hidden_layer_activation))  # For L-1 activation layer \n",
    "        \n",
    "        self.softmax = Softmax() # L-th layer \n",
    "        \n",
    "    \n",
    "    def get_cost(self, Y, A):\n",
    "        L2_regularization_cost = 0 \n",
    "        for l in range(self.num_layers):\n",
    "            L2_regularization_cost += np.sum(np.square(self.linear_layer[l].W))\n",
    "            \n",
    "        L2_regularization_cost *=  self.l2_regularization_param/(2* self.n)\n",
    "        \n",
    "        return -np.mean(Y * np.log(A.T+ 1e-8)) +  L2_regularization_cost\n",
    "    \n",
    "    \n",
    "    def forward(self, X):\n",
    "        A = np.copy(X).T\n",
    "        assert(A.shape[0] == self.num_features)\n",
    "        \n",
    "        for l in range(self.num_layers - 1):\n",
    "            Z = self.linear_layer[l](A) \n",
    "            A = self.activation_layer[l](Z)\n",
    "\n",
    "        Z = self.linear_layer[-1](A)  \n",
    "        A = self.softmax(Z)\n",
    "        return A\n",
    "    \n",
    "    \n",
    " \n",
    "    def fit(self, X, Y, learning_rate=0.01, n_iterations=2500):\n",
    "        \n",
    "        np.random.seed(1)\n",
    "        self.n = X.shape[0]\n",
    "        for loop in range(n_iterations):\n",
    "            A = self.forward(X)\n",
    "            cost = self.get_cost( Y, A )\n",
    "            dZ = self.softmax.backward(Y.T)\n",
    "\n",
    "        \n",
    "            dAPrev, dW, db = self.linear_layer[-1].backward(self.n, dZ)\n",
    "            self.linear_layer[-1].update(dW, db, self.n,  learning_rate, self.l2_regularization_param )\n",
    "\n",
    "            for l in range(self.num_layers - 1, 0, -1):\n",
    "                dZ = self.activation_layer[l-1].backward(dAPrev)\n",
    "                dAPrev, dW, db = self.linear_layer[l-1].backward(self.n, dZ)\n",
    "                \n",
    "                self.linear_layer[l-1].update(dW, db, self.n,  learning_rate, self.l2_regularization_param )\n",
    "\n",
    "\n",
    "            if loop % 100 == 0:\n",
    "                print(\"Cost: \", cost, \"Train Accuracy:\", self.predict(X, Y))\n",
    " \n",
    "            if loop % 10 == 0:\n",
    "                self.costs.append(cost)\n",
    " \n",
    "    def predict(self, X, Y, filePrint=False):\n",
    "\n",
    "        A = self.forward(X)\n",
    "        y_hat = np.argmax(A, axis=0)\n",
    "        Y = np.argmax(Y, axis=1)\n",
    "        accuracy = (y_hat == Y).mean()\n",
    "        \n",
    "        if filePrint:\n",
    "            original_stdout = sys.stdout # Save a reference to the original standard output\n",
    "\n",
    "            with open('1505014_report.txt', 'w') as f:\n",
    "                sys.stdout = f # Change the standard output to the file we created.\n",
    "                print('Number of hidden layers ', self.num_layers - 1 )\n",
    "                print('List of mis-classified samples')\n",
    "                t = PrettyTable(['sample no.', 'feature values', 'actual class', 'predicted class'])\n",
    "\n",
    "                for i in range(len(Y)):\n",
    "                    y_pred = y_hat[i]\n",
    "                    if y_pred != Y[i]:\n",
    "                        t.add_row([i, x_test[i,:],  y_test[i], y_pred])\n",
    "                print(t)\n",
    "                print('Accurary for Neural Network ')\n",
    "                print(accuracy*100, '%')\n",
    "                sys.stdout = original_stdout # Reset the standard output to its original value\n",
    "\n",
    "        return accuracy * 100\n",
    " \n",
    "    def plot_cost(self):\n",
    "        plt.figure()\n",
    "        plt.plot(np.arange(len(self.costs)), self.costs)\n",
    "        plt.xlabel(\"epochs\")\n",
    "        plt.ylabel(\"cost\")\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.23550865 -1.38630451 -1.33573181 -1.3946362 ]\n",
      " [-0.46809072 -0.48626348 -0.47001247 -0.4894833 ]\n",
      " [ 1.22327934  1.30075385  1.32246657  1.30438191]\n",
      " ...\n",
      " [-1.38168187 -1.37596632 -1.35725188 -1.40248016]\n",
      " [-0.58225414 -0.46142325 -0.50124281 -0.48931872]\n",
      " [ 1.33308563  1.31153135  1.31478377  1.34229115]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "missing_values = [\"n/a\", \"na\", \"--\",\"NA\",\"N/A\",\"?\"]\n",
    "\n",
    "    \n",
    "df = pd.read_csv( trainFile, na_values = missing_values, delimiter=r'\\s+', header=None)\n",
    "\n",
    "\n",
    "train_x, train_y = np.array(df.iloc[:,:-1].values.tolist()), np.array(df.iloc[:,-1].values.tolist() )\n",
    "numFeature, numClass = df.shape[1]-1, len(set(train_y))\n",
    "\n",
    "dfTest = pd.read_csv( testFile, na_values = missing_values, delimiter=r'\\s+', header=None)\n",
    "test_x, test_y = np.array(dfTest.iloc[ :,:-1].values.tolist()), np.array(dfTest.iloc[:,-1].values.tolist() )\n",
    "\n",
    "train_x1, train_y1, test_x1, test_y1 = pre_process_data(train_x, train_y, test_x, test_y)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x's shape: (500, 4)\n",
      "test_x's shape: (500, 4)\n",
      "Cost:  0.35909778599374265 Train Accuracy: 49.8\n",
      "Cost:  0.03737680307684004 Train Accuracy: 100.0\n",
      "Cost:  0.024603243422458523 Train Accuracy: 100.0\n",
      "Cost:  0.02097661320480911 Train Accuracy: 100.0\n",
      "Cost:  0.019578391508890194 Train Accuracy: 100.0\n",
      "Train Accuracy: 100.0\n",
      "Test Accuracy: 100.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "print(\"train_x's shape: \" + str(train_x1.shape))\n",
    "print(\"test_x's shape: \" + str(test_x1.shape))\n",
    "\n",
    "layers_dims = [] # excluding input, output layer \n",
    "\n",
    "ann = ANN(layers_dims, numFeature, numClass, ACTIVATION_ENUM.LEAKY_RELU , l2_regularization_param = 0.1 ) #pass l2_param = 0 if no regularization needed \n",
    "ann.fit(train_x1, train_y1, learning_rate=0.9, n_iterations=500)\n",
    "print(\"Train Accuracy:\", ann.predict(train_x1, train_y1))\n",
    "print(\"Test Accuracy:\", ann.predict(test_x1, test_y1, filePrint=True))\n",
    "ann.plot_cost()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
